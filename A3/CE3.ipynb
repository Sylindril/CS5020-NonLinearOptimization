{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coding Exercise 3\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "Choices:\n",
    "Slope = m = 5\n",
    "Intercept = c = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from IPython.display import HTML\n",
    "m =  5\n",
    "c =  -5\n",
    "matplotlib.rcParams['animation.embed_limit'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.uniform(-1,1,100)\n",
    "a = np.linspace(-5,5,100)\n",
    "b = m*a + c + u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x, a = a, b = b):\n",
    "    return np.array([np.mean(-2*a*(b - a*x[0] - x[1])), np.mean(-2*(b - x[0]*a - x[1]))])\n",
    "\n",
    "def stoch_grad(x, a = a, b = b, S = 10):\n",
    "    choice_pool = np.random.randint(0,len(a),S)\n",
    "    return grad(x, a = a[choice_pool],b = b[choice_pool])\n",
    "\n",
    "def grad_descent(alpha, T, x0 = np.array([3,3])):\n",
    "    xt = x0\n",
    "    x_arr = [xt]\n",
    "    for i in range(T):\n",
    "        xt = xt - alpha*grad(xt)\n",
    "        x_arr.append(xt)\n",
    "    x_diff = [np.linalg.norm(xt-np.array([m,c])) for xt in x_arr]\n",
    "    return xt,x_arr, x_diff\n",
    "\n",
    "def stoch_grad_descent(alpha, T, x0 = np.array([3,3])):\n",
    "    xt = x0\n",
    "    x_arr = [xt]\n",
    "    for i in range(T):\n",
    "        xt = xt - alpha*stoch_grad(xt)\n",
    "        x_arr.append(xt)\n",
    "    x_diff = [np.linalg.norm(xt-np.array([m,c])) for xt in x_arr]\n",
    "    return xt,x_arr, x_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (a) Alpha = 0.01, Point Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa,x_arr_abs, abs_err = grad_descent(0.01,T)\n",
    "xs,x_arr_stoch, stoch_err = stoch_grad_descent(0.01,T)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(abs_err, color = 'blue', label = 'Gradient Descent')\n",
    "plt.plot(stoch_err, color = 'green', label = 'Stochastic Gradient Descent')\n",
    "plt.ylabel('||xt - x*||')\n",
    "plt.xlabel('Step Count')\n",
    "plt.title('Comparing Gradient Descent with Stochastic Gradient Descent for alpha = 0.01')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (b) Alpha = 0.1, Point Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.1,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.1,T)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(abs_err, color = 'blue', label = 'Gradient Descent')\n",
    "plt.plot(stoch_err, color = 'green', label = 'Stochastic Gradient Descent')\n",
    "plt.ylabel('||xt - x*||')\n",
    "plt.xlabel('Step Count')\n",
    "plt.title('Comparing Gradient Descent with Stochastic Gradient Descent for alpha = 0.1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With alpha = 0.1, the convergence for SGD is much worse than it is for alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### (c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (6.4,4.8))\n",
    "alpha = 0.1\n",
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.1,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.1,T)\n",
    "\n",
    "\n",
    "ax1.set_xlim(-6,6)\n",
    "ax1.set_ylim(-35,25)\n",
    "ax2.set_xlim(-6,6)\n",
    "ax2.set_ylim(-35,25)\n",
    "ax1.plot(a, b, 'o', lw = 3, label='Data')[0]\n",
    "line1 = ax2.plot([], [], color = 'red', lw = 3, label='Gradient Descent')[0]\n",
    "line2 = ax2.plot([], [], color = 'green', lw = 3, label='Stochastic Gradient Descent')[0]\n",
    "points = ax2.plot(a, b, 'o', lw = 3, label='Data', alpha = 0.4)[0]\n",
    "ax1.set_ylabel('b(or label)')\n",
    "ax1.set_xlabel('a(or features)')\n",
    "ax1.set_title('Dataset')\n",
    "ax2.set_ylabel('b(or label)')\n",
    "ax2.set_xlabel('a(or features)')\n",
    "ax2.set_title('Predictor')\n",
    "plt.legend()\n",
    "\n",
    "def init():\n",
    "    line1.set_data([],[])\n",
    "    line2.set_data([],[])\n",
    "    return line1, line2 \n",
    "\n",
    "def animate(i):\n",
    "    x = a\n",
    "    y = x_arr_abs[i][0]*x + x_arr_abs[i][1]\n",
    "    line1.set_data(x, y)\n",
    "    y = x_arr_stoch[i][0]*x + x_arr_stoch[i][1]\n",
    "    line2.set_data(x, y)\n",
    "\n",
    "    return line1, line2 \n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, interval=100, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (6.4,4.8))\n",
    "alpha = 0.01\n",
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.1,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.1,T)\n",
    "\n",
    "\n",
    "ax1.set_xlim(-6,6)\n",
    "ax1.set_ylim(-35,25)\n",
    "ax2.set_xlim(-6,6)\n",
    "ax2.set_ylim(-35,25)\n",
    "ax1.plot(a, b, 'o', lw = 3, label='Data')[0]\n",
    "line1 = ax2.plot([], [], color = 'red', lw = 3, label='Gradient Descent')[0]\n",
    "line2 = ax2.plot([], [], color = 'green', lw = 3, label='Stochastic Gradient Descent')[0]\n",
    "points = ax2.plot(a, b, 'o', lw = 3, label='Data', alpha = 0.4)[0]\n",
    "ax1.set_ylabel('b(or label)')\n",
    "ax1.set_xlabel('a(or features)')\n",
    "ax1.set_title('Dataset')\n",
    "ax2.set_ylabel('b(or label)')\n",
    "ax2.set_xlabel('a(or features)')\n",
    "ax2.set_title('Predictor')\n",
    "plt.legend()\n",
    "\n",
    "def init():\n",
    "    line1.set_data([],[])\n",
    "    line2.set_data([],[])\n",
    "    return line1, line2 \n",
    "\n",
    "def animate(i):\n",
    "    x = a\n",
    "    y = x_arr_abs[i][0]*x + x_arr_abs[i][1]\n",
    "    line1.set_data(x, y)\n",
    "    y = x_arr_stoch[i][0]*x + x_arr_stoch[i][1]\n",
    "    line2.set_data(x, y)\n",
    "\n",
    "    return line1, line2 \n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, interval=100, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.1,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.1,T)\n",
    "x0_abs = [x_arr_abs[i][0] for i in range(len(x_arr_abs))]\n",
    "x1_abs = [x_arr_abs[i][1] for i in range(len(x_arr_abs))]\n",
    "x0_stoch = [x_arr_stoch[i][0] for i in range(len(x_arr_stoch))]\n",
    "x1_stoch = [x_arr_stoch[i][1] for i in range(len(x_arr_stoch))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "\n",
    "def loss_func(x,y, a = a , b = b):\n",
    "    N = len(a)\n",
    "    loss = 0\n",
    "    for i in range(N):\n",
    "        loss += (b[i] - a[i]*x - y)**2\n",
    "    return loss/N\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "X,Y = np.meshgrid(np.linspace(-6,6,30), np.linspace(-6,6,30))\n",
    "Z = loss_func(X,Y)\n",
    "ax1.contour3D(X,Y,Z,50, cmap='binary')\n",
    "ax1.set_xlim3d([-6,6])\n",
    "ax1.set_ylim3d([-6,6])\n",
    "ax1.set_zlim3d([0,1000])\n",
    "\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_zlabel('Loss Function')\n",
    "ax1.set_title(\"Gradient Descent over Loss Function\")\n",
    "\n",
    "\n",
    "line1 = [ax1.plot([], [], [], 'ro', alpha=0.4)[0], ax1.plot([], [], [], 'bo', alpha=0.4)[0]]\n",
    "\n",
    "def update_points(i):\n",
    "    \n",
    "    line1[0].set_data(x0_abs[:i+1], x1_abs[:i+1])\n",
    "    line1[0].set_3d_properties(loss_func(x0_abs[:i+1], x1_abs[:i+1]))\n",
    "    line1[1].set_data(x0_stoch[:i+1], x1_stoch[:i+1])\n",
    "    line1[1].set_3d_properties(loss_func(x0_stoch[:i+1], x1_stoch[:i+1]))\n",
    "    return line1\n",
    "\n",
    "def init():\n",
    "    for line in line1:\n",
    "        line.set_data([],[])\n",
    "        line.set_3d_properties([])\n",
    "    return line1\n",
    "\n",
    "anim = FuncAnimation(fig, update_points, init_func=init, frames=T,  blit=True)\n",
    "HTML(anim.to_jshtml())\n",
    "\n",
    "print(\"Legend: Red -> Gradient Descent, Blue -> Stochastic Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.01,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.01,T)\n",
    "x0_abs = [x_arr_abs[i][0] for i in range(len(x_arr_abs))]\n",
    "x1_abs = [x_arr_abs[i][1] for i in range(len(x_arr_abs))]\n",
    "x0_stoch = [x_arr_stoch[i][0] for i in range(len(x_arr_stoch))]\n",
    "x1_stoch = [x_arr_stoch[i][1] for i in range(len(x_arr_stoch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "\n",
    "ax1.contour3D(X,Y,Z,50, cmap='binary')\n",
    "ax1.set_xlim3d([-6,6])\n",
    "ax1.set_ylim3d([-6,6])\n",
    "ax1.set_zlim3d([0,1000])\n",
    "\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_zlabel('Loss Function')\n",
    "ax1.set_title(\"Gradient Descent over Loss Function\")\n",
    "\n",
    "line1 = [ax1.plot([], [], [], 'ro', alpha=0.4)[0], ax1.plot([], [], [], 'bo', alpha=0.4)[0]]\n",
    "\n",
    "def update_points(i):\n",
    "    \n",
    "    line1[0].set_data(x0_abs[:i+1], x1_abs[:i+1])\n",
    "    line1[0].set_3d_properties(loss_func(x0_abs[:i+1], x1_abs[:i+1]))\n",
    "    line1[1].set_data(x0_stoch[:i+1], x1_stoch[:i+1])\n",
    "    line1[1].set_3d_properties(loss_func(x0_stoch[:i+1], x1_stoch[:i+1]))\n",
    "    return line1\n",
    "\n",
    "def init():\n",
    "    for line in line1:\n",
    "        line.set_data([],[])\n",
    "        line.set_3d_properties([])\n",
    "    return line1\n",
    "\n",
    "anim = FuncAnimation(fig, update_points, init_func=init, frames=T,  blit=True)\n",
    "HTML(anim.to_jshtml())\n",
    "print(\"Legend: Red -> Gradient Descent, Blue -> Stochastic Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.1,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.1,T)\n",
    "x0_abs = [x_arr_abs[i][0] for i in range(len(x_arr_abs))]\n",
    "x1_abs = [x_arr_abs[i][1] for i in range(len(x_arr_abs))]\n",
    "x0_stoch = [x_arr_stoch[i][0] for i in range(len(x_arr_stoch))]\n",
    "x1_stoch = [x_arr_stoch[i][1] for i in range(len(x_arr_stoch))]\n",
    "\n",
    "@np.vectorize\n",
    "def unVecGradient(x, y, a = a, b = b):\n",
    "    p1 = p2 = 0\n",
    "    N = len(a)\n",
    "    for i in range(N):\n",
    "        p1 += -2*a[i]*(b[i] - a[i]*x - y)\n",
    "        p2 += -2*(b[i] - a[i]*x - y)\n",
    "    return p1/N, p2/N\n",
    "\n",
    "\n",
    "M, N = unVecGradient(X,Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "axis = plt.axes(xlim =(-6, 6), ylim =(-6, 6))\n",
    "line1 = axis.plot([], [], 'bo', alpha=0.4, label='Gradient Descent')[0]\n",
    "line2 = axis.plot([], [], 'go', alpha=0.4, label='Stochastic Gradient Descent')[0]\n",
    "line3 = axis.quiver(X,Y,M,N, label='Quiver Plot')\n",
    "axis.legend(loc = 'upper left')\n",
    "\n",
    "def init():\n",
    "    return line1, line2\n",
    "\n",
    "def animate(i):\n",
    "    axis.plot(x0_abs[i], x1_abs[i],'bo')\n",
    "    axis.plot(x0_stoch[i], x1_stoch[i], 'go')\n",
    "    return line1, line2\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "xa, x_arr_abs,abs_err = grad_descent(0.01,T)\n",
    "xs, x_arr_stoch,stoch_err = stoch_grad_descent(0.01,T)\n",
    "x0_abs = [x_arr_abs[i][0] for i in range(len(x_arr_abs))]\n",
    "x1_abs = [x_arr_abs[i][1] for i in range(len(x_arr_abs))]\n",
    "x0_stoch = [x_arr_stoch[i][0] for i in range(len(x_arr_stoch))]\n",
    "x1_stoch = [x_arr_stoch[i][1] for i in range(len(x_arr_stoch))]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "axis = plt.axes(xlim =(-6, 6), ylim =(-6, 6))\n",
    "line1 = axis.plot([], [], 'bo', alpha=0.4, label='Gradient Descent')[0]\n",
    "line2 = axis.plot([], [], 'go', alpha=0.4, label='Stochastic Gradient Descent')[0]\n",
    "line3 = axis.quiver(X,Y,M,N, label='Quiver Plot')\n",
    "axis.legend(loc = 'upper left')\n",
    "\n",
    "def init():\n",
    "    return line1, line2\n",
    "\n",
    "def animate(i):\n",
    "    axis.plot(x0_abs[i], x1_abs[i],'bo')\n",
    "    axis.plot(x0_stoch[i], x1_stoch[i], 'go')\n",
    "    return line1, line2\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [-1]*50 + [1]*50\n",
    "b = np.array(b)\n",
    "a1 = np.hstack((np.random.uniform( 30, 45,50), np.random.uniform( 55, 70,50)))\n",
    "a2 = np.hstack((np.random.uniform(125,145,50), np.random.uniform(155,180,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim =(25, 75), ylim =(120, 185))\n",
    "ax.scatter(a1[:50], a2[:50])\n",
    "ax.scatter(a1[50:], a2[50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(x1, x2, x3, a1=a1, a2=a2, b=b):\n",
    "    N = len(a1)\n",
    "    return np.sum(np.log(1+np.exp(-b*(x1*a1 + x2*a2 + x3))))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logRegression(x1, x2, x3, a1=a1, a2=a2, b=b):\n",
    "    N = len(b)\n",
    "    grad_x1 = grad_x2 = grad_x3 = 0\n",
    "    for i in range(N):\n",
    "        grad_x1 += 1/(1+np.exp(b[i]*(x1*a1[i] + x2*a2[i] + x3))) * -b[i]*a1[i]\n",
    "        grad_x2 += 1/(1+np.exp(b[i]*(x1*a1[i] + x2*a2[i] + x3))) * -b[i]*a2[i]\n",
    "        grad_x3 += 1/(1+np.exp(b[i]*(x1*a1[i] + x2*a2[i] + x3))) * -b[i]\n",
    "    return grad_x1/N, grad_x2/N, grad_x3/N\n",
    "def stoch_grad_logRegression(x1, x2, x3, a1=a1, a2=a2, b=b, S=10):\n",
    "    choice_pool = np.random.randint(0,100,S)\n",
    "    return grad_logRegression(x1, x2, x3, a1=a1[choice_pool], a2=a2[choice_pool], b=b[choice_pool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descL(alpha, T):\n",
    "    x1_list = np.zeros(T)\n",
    "    x2_list = np.zeros(T)\n",
    "    x3_list = np.zeros(T)\n",
    "    x1_list[0] = -3\n",
    "    x2_list[0] = 0\n",
    "    x3_list[0] = -70\n",
    "    for i in range(1,T):\n",
    "        x1_grad, x2_grad, x3_grad = grad_logRegression(x1_list[i-1],x2_list[i-1],x3_list[i-1])\n",
    "        x1_list[i] = x1_list[i-1] - alpha * x1_grad\n",
    "        x2_list[i] = x2_list[i-1] - alpha * x2_grad\n",
    "        x3_list[i] = x3_list[i-1] - alpha * x3_grad\n",
    "    return x1_list, x2_list, x3_list\n",
    "\n",
    "def Sgrad_descL(alphat, T, S = 10):\n",
    "    x1_list = np.empty(T)\n",
    "    x2_list = np.empty(T)\n",
    "    x3_list = np.empty(T)\n",
    "    x1_list[0] = -3\n",
    "    x2_list[0] = 0\n",
    "    x3_list[0] = -70\n",
    "    for i in range(1,T):\n",
    "        x1_grad, x2_grad, x3_grad = stoch_grad_logRegression(x1_list[i-1],x2_list[i-1],x3_list[i-1], S=S)\n",
    "        x1_list[i] = x1_list[i-1] - alpha * x1_grad\n",
    "        x2_list[i] = x2_list[i-1] - alpha * x2_grad\n",
    "        x3_list[i] = x3_list[i-1] - alpha * x3_grad\n",
    "    return x1_list, x2_list, x3_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For alpha = 0.01, T = 1000\n",
    "plt.figure(figsize = (9.4,4.8))\n",
    "T = 1000\n",
    "alpha = 0.01\n",
    "x1_arr_abs, x2_arr_abs, x3_arr_abs = grad_descL(alpha, T = T)\n",
    "x1_arr_stoch, x2_arr_stoch, x3_arr_stoch = Sgrad_descL(alpha,T)\n",
    "\n",
    "ax1 =  plt.subplot(1,3,1)\n",
    "plt.scatter(range(1000),x1_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x1_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x1\")\n",
    "plt.legend()\n",
    "\n",
    "ax2 =  plt.subplot(1,3,2)\n",
    "plt.scatter(range(1000),x2_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x2_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x2\")\n",
    "plt.legend()\n",
    "\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "plt.scatter(range(1000),x3_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x3_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x3\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For alpha = 0.1, T = 1000\n",
    "print(\"Alpha = 0.1, Divergence is seen\")\n",
    "plt.figure(figsize = (9.4,4.8))\n",
    "T = 1000\n",
    "alpha = 0.1\n",
    "x1_arr_abs, x2_arr_abs, x3_arr_abs = grad_descL(alpha, T = T)\n",
    "x1_arr_stoch, x2_arr_stoch, x3_arr_stoch = Sgrad_descL(alpha,T)\n",
    "\n",
    "ax1 =  plt.subplot(1,3,1)\n",
    "plt.scatter(range(1000),x1_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x1_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x1\")\n",
    "plt.legend()\n",
    "\n",
    "ax2 =  plt.subplot(1,3,2)\n",
    "plt.scatter(range(1000),x2_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x2_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x2\")\n",
    "plt.legend()\n",
    "\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "plt.scatter(range(1000),x3_arr_abs, label = 'GD')\n",
    "plt.scatter(range(1000),x3_arr_stoch, label = 'SGD', alpha = 0.4)\n",
    "plt.title(\"x3\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "alpha = 0.1\n",
    "x1_arr_abs, x2_arr_abs, x3_arr_abs = grad_descL(alpha, T = T)\n",
    "x1_arr_stoch, x2_arr_stoch, x3_arr_stoch = Sgrad_descL(alpha, T)\n",
    "\n",
    "print(\"Running: Gradient Calculated\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim =(25, 75), ylim =(120, 185))\n",
    "x = np.linspace(30,70, 100)\n",
    "ax.scatter(a1[:50],a2[:50],label=\"Children\")\n",
    "ax.scatter(a1[50:],a2[50:],label=\"Adults\")\n",
    "line = [ax.plot([],[], 'b', label='Gradient Descent')[0], ax.plot([],[], 'g', label='Stochastic Gradient Descent')[0]]\n",
    "ax.set_xlabel(\"Weight a(1)\")\n",
    "ax.set_ylabel(\"Height a(2)\")\n",
    "ax.legend(loc = 'upper left')\n",
    "\n",
    "def animate(i):\n",
    "    line[0].set_data(x, (-x*x1_arr_abs[i]-x3_arr_abs[i])/x2_arr_abs[i])\n",
    "    line[1].set_data(x, (-x*x1_arr_stoch[i]-x3_arr_stoch[i])/x2_arr_stoch[i])\n",
    "    return line\n",
    "\n",
    "def init():\n",
    "    return line\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, interval=100, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can clearly see that the above optimizer doesn't converge for alpha = 0.1\n",
    "\n",
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "alpha = 0.01\n",
    "\n",
    "x1_arr_abs, x2_arr_abs, x3_arr_abs = grad_descL(alpha, T = T)\n",
    "x1_arr_stoch, x2_arr_stoch, x3_arr_stoch = Sgrad_descL(alpha, T)\n",
    "\n",
    "print(\"Running: Gradient Calculated\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim =(25, 75), ylim =(120, 185))\n",
    "x = np.linspace(30,70, 100)\n",
    "ax.scatter(a1[:50],a2[:50],label=\"Children\")\n",
    "ax.scatter(a1[50:],a2[50:],label=\"Adults\")\n",
    "line = [ax.plot([],[], 'b', label='Gradient Descent')[0], ax.plot([],[], 'g', label='Stochastic Gradient Descent')[0]]\n",
    "ax.set_xlabel(\"Weight a(1)\")\n",
    "ax.set_ylabel(\"Height a(2)\")\n",
    "ax.legend(loc = 'upper left')\n",
    "\n",
    "def animate(i):\n",
    "    line[0].set_data(x, (-x*x1_arr_abs[i]-x3_arr_abs[i])/x2_arr_abs[i])\n",
    "    line[1].set_data(x, (-x*x1_arr_stoch[i]-x3_arr_stoch[i])/x2_arr_stoch[i])\n",
    "    return line\n",
    "\n",
    "def init():\n",
    "    return line\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=T, interval=100, blit=True)\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc0c4256163a8ebf9e83b90db299ae2aa7c09ef8eab8f4bfdd4478f5cddc9f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
